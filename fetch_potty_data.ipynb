{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455911cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import datetime\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import subprocess\n",
    "\n",
    "def extract_park_geo(park_doc):\n",
    "    p = dict()\n",
    "    for var in (\"latitude\", \"longitude\", \"signName\", \"parkId\"):\n",
    "        m = re.search(f\"var {var} = ([^;]+);\", park_doc)\n",
    "        if not m:\n",
    "            print(f\"Missing {var}\")\n",
    "            p[var] = None\n",
    "        else:\n",
    "            p[var] = m.group(1).strip('\"')\n",
    "        \n",
    "    return p\n",
    "\n",
    "\n",
    "def prefetch_local_pages(root_dir):\n",
    "    cmd = \"\"\"wget --recursive --level 2 https://www.nycgovparks.org/facilities/bathrooms \n",
    "    -I parks,facilities --reject pdf,jpg,video --wait 0.1 --timeout 1 --tries 3\"\"\".split()\n",
    "    os.chdir(root_dir)\n",
    "    print(cmd)\n",
    "    print(subprocess.call(cmd))\n",
    "\n",
    "\n",
    "class LocalScraper:\n",
    "    ROOT_DIR = 'data/www.nycgovparks.org/'\n",
    "\n",
    "    def fetch_potty_doc(self):\n",
    "        potty_page = f\"{self.ROOT_DIR}/facilities/bathrooms\"\n",
    "        return open(potty_page).read()\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def fetch_park_geo(self, park_id):\n",
    "        park_dir = f\"{self.ROOT_DIR}/parks/{park_id}\"\n",
    "        if not os.path.isdir(park_dir):\n",
    "            print(f\"{park_id} not a dir\")\n",
    "            return {}\n",
    "        if not os.path.isfile(f\"{park_dir}/map\"):\n",
    "            print(f\"{park_id} no map\")\n",
    "            return {}\n",
    "    \n",
    "        return extract_park_geo(open(f\"{park_dir}/map\").read())\n",
    "    \n",
    "    \n",
    "class OnlineScraper:\n",
    "    ROOT_URL = 'https://www.nycgovparks.org'\n",
    "    \n",
    "    def fetch_potty_doc(self):\n",
    "        potty_page = f\"{self.ROOT_URL}/facilities/bathrooms\"\n",
    "        return requests.get(potty_page).content\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def fetch_park_geo(self, park_id):\n",
    "        park_page = f\"{self.ROOT_URL}/parks/{park_id}/map\"\n",
    "        return extract_park_geo(requests.get(park_page).content.decode('utf-8'))\n",
    "\n",
    "\n",
    "def potty_list(doc):\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "        \n",
    "    potties = []\n",
    "    \n",
    "    tabnav = soup.find('ul', class_='nav-tabs').find_all('li')\n",
    "    boroughs = { li.find('a').attrs['href']: li.find('a').text \n",
    "                 for li in tabnav }\n",
    "    \n",
    "    tab = soup.find(class_='tab-content')\n",
    "    for pane in tab.find_all('div', class_='tab-pane'):\n",
    "        boro = boroughs[f\"#{pane.attrs['id']}\"]\n",
    "        rows = pane.find_all('tr')\n",
    "        \n",
    "        for r in rows:\n",
    "            p = { 'park_borough': boro }\n",
    "            td = r.find_all('td')\n",
    "            if not td:\n",
    "                continue\n",
    "        \n",
    "            p['potty_name'] = td[0].text\n",
    "\n",
    "            park = list(td[1].children)\n",
    "            if not park:\n",
    "                continue\n",
    "            park_link = park[0]\n",
    "            p['park_name'] = park_link.text\n",
    "            p['park_page'] = park_link.attrs['href'][7:]\n",
    "            \n",
    "            if len(park) > 2:\n",
    "                p['potty_location'] = park[2]\n",
    "            else:\n",
    "                p['potty_location'] = park[1].text\n",
    "            \n",
    "            p['accessible'] = (len(td) > 3 and td[3].find('img', alt='Accessible') is not None)\n",
    "            \n",
    "            potties.append(p)\n",
    "        \n",
    "    return potties\n",
    "\n",
    "\n",
    "def potty_rec(potty):\n",
    "    return {\n",
    "        'ID': f\"P{''.join(random.choices(string.ascii_uppercase + string.digits, k=5))}\",\n",
    "        'Borough': potty['park_borough'],\n",
    "        'Name': potty['potty_name'],\n",
    "        'Park Name': potty['park_name'],\n",
    "        'Human-readable location': potty['potty_location'],\n",
    "        'Lat': potty['park_geo'].get('latitude', ''),\n",
    "        'Lon': potty['park_geo'].get('longitude', ''),\n",
    "        'Location URL': f\"https://www.nycgovparks.org/parks/{potty['park_page']}\",\n",
    "        'Type': '',\n",
    "        'Number of stalls': '',\n",
    "        'Accessible': potty['accessible'],\n",
    "        'Hours': '',\n",
    "        'Last updated': datetime.datetime.now().isoformat(),\n",
    "        'Scrape date': datetime.date.today().isoformat(),\n",
    "    }\n",
    "\n",
    "\n",
    "def write_csv(output_fh, potties):\n",
    "    writer = csv.DictWriter(output_fh, fieldnames=[\n",
    "        'ID', 'Borough', 'Name', 'Park Name', 'Human-readable location', 'Lat', 'Lon', \n",
    "        'Location URL', 'Type', 'Number of stalls', 'Accessible',\n",
    "        'Hours', 'Last updated', 'Scrape date'\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    for p in potties:\n",
    "        writer.writerow(potty_rec(p))\n",
    "    \n",
    "\n",
    "def get_potties_with_park_geo(scraper):\n",
    "    doc = scraper.fetch_potty_doc()\n",
    "    ps = potty_list(doc)\n",
    "\n",
    "    for p in ps:\n",
    "        p['park_geo'] = scraper.fetch_park_geo(p['park_page'])\n",
    "\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefetch_local_pages('/Users/ivancho/src/pottyproject/data/')\n",
    "# scraper = LocalScraper()\n",
    "\n",
    "scraper = OnlineScraper()\n",
    "    \n",
    "ps = get_potties_with_park_geo(scraper)\n",
    "\n",
    "len(ps), len([p for p in ps if p['park_geo']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data we scraped\n",
    "with open('potty_raw.csv', 'w') as fh:\n",
    "    write_csv(fh, ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
